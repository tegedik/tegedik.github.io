[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tahir Enes Gedik",
    "section": "",
    "text": "Sociologist interested in quantitative analysis and statistical methods in the social sciences."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 29, 2022\n\n\nBourdieu and Boudon: Why One is Revered and the Other Neglected in Turkey?\n\n\nT.E.G. \n\n\n\n\nAug 29, 2021\n\n\nTrouble with Multinomial Logistic Regression Tables\n\n\nT.E.G. \n\n\n\n\nJun 15, 2020\n\n\nAchen ve Üç Kuralı\n\n\nT.E.G. \n\n\n\n\nMay 3, 2020\n\n\nNonparametric Tests and Why I am Wary of Using Them\n\n\nT.E.G. \n\n\n\n\nMar 31, 2019\n\n\nPoor Man’s Galton Board\n\n\nT.E.G. \n\n\n\n\nDec 7, 2018\n\n\nSorcerer’s Apprentice Approach to Statistical Analysis\n\n\nT.E.G. \n\n\n\n\nFeb 6, 2018\n\n\nMaximum Likelihood Estimation: Finding the Top of a Hill\n\n\nT.E.G. \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nonp/index.html",
    "href": "posts/nonp/index.html",
    "title": "Nonparametric Tests and Why I am Wary of Using Them",
    "section": "",
    "text": "If you are following online communities where people ask for statistics advice, you’ve probably seen a type of question popping up from time to time: “I am doing an analysis using parametric test X, however, (assumption check) test A reveals that assumption B is violated. What should I do?” (As you may guess, the violated assumption is usually the normality assumption). And people start recommending a number of textbook solutions. One such solution is to stop using the parametric test and to prefer a nonparametric equivalent. For example, if you use \\(t\\) test and violate the “normality” assumption, some would say that you should use Mann-Whitney U test. And that’s it.\nI think these questions are problematic for two reasons. First, they are asking for recipe-like solutions, which could be justified by a few references to the extant literature, without thinking about the specific issues related to the analysis at hand. Second, they encourage binary thinking: “If X is violated, do Y, if not do Z.” It would be better to think about the possible consequences of such violations and act accordingly. Otherwise you might be choosing between two faulty approaches.\nI am not against using nonparametric tests per se (they might even have desirable properties), but the way decisions are made regarding these tests.  In my case, the foremost reason why I hesitate using nonparametric tests is that they do not generalize well to complex models. It is not an issue to use a nonparametric test comparing two groups. But what happens if you want to estimate cross-level interactions and random effects? As this answer in Cross Validated indicates, we can think parametric assumptions as simplifying heuristics:\n\nMore generally, parametric procedures provide a way of imposing structure on otherwise unstructured problems. This is very useful and can be viewed as a kind of simplifying heuristic rather than a belief that the model is literally true. Take for instance the problem of predicting a continuous response \\(y\\) based on a vector of predictors \\(x\\) using some regression function \\(f\\) (even assuming that such a function exists is a kind of parametric restriction). If we assume absolutely nothing about \\(f\\) then it’s not at all clear how we might proceed in estimating this function. The set of possible answers that we need to search is just too large. But if we restrict the space of possible answers to (for instance) the set of linear functions \\(f(x)=\\sum_{j=1}^p \\beta_j x_j\\) then we can actually start making progress. We don’t need to believe that the model holds exactly, we are just making an approximation due to the need to arrive at some answer, however imperfect.\n\nThus, such a simplifying heuristic (e.g., specifiying a regression function) would enable one to estimate the parameters of a complex model.11 In his Regression Modeling Strategies (2015, 359), Harrell argues that semiparametric models (e.g., proportional odds logistic regression) have many advantages such as “robustness and freedom from all distributional assumptions for Y conditional on any given set of predictors.” I would prefer such an approach if needed.\n\n\n\n\n\n\nReferences\n\nHarrell, Frank E. 2015. Regression Modeling Strategies. Springer International Publishing.\n\nCitationBibTeX citation:@online{2020,\n  author = {, T.E.G.},\n  title = {Nonparametric {Tests} and {Why} {I} Am {Wary} of {Using}\n    {Them}},\n  date = {2020-05-03},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nT.E.G. 2020. “Nonparametric Tests and Why I Am Wary of Using\nThem.” May 3, 2020."
  },
  {
    "objectID": "posts/multinom/index.html",
    "href": "posts/multinom/index.html",
    "title": "Trouble with Multinomial Logistic Regression Tables",
    "section": "",
    "text": "I work mostly with binary and ordinal outcomes. So, I rarely use multinomial logistic regression but yesterday I had to, and it proved tricky to create tables for model results. Basically, multinomial logistic regression generalizes logistic regression to nominal variables with multiple categories. It is preferred when these categories are unordered but also used for ordinal outcomes when proportional odds assumption is violated. It is easy to estimate, but not as straightforward to interpret as binary logistic regression. And it might not be practical when your outcome has many categories. In R, you can use multinom function from {nnet} package to estimate multinomial logistic regression models:11 I use the data from the example on IDRE UCLA: https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/.\n\ndf &lt;- read.dta(\"https://stats.idre.ucla.edu/stat/data/hsbdemo.dta\") \nmultinom_fit &lt;- multinom(prog ~ ses + female + write, \n                         data = df, trace=FALSE) # trace=FALSE to suppress convergence info.\nsummary(multinom_fit)\n\nCall:\nmultinom(formula = prog ~ ses + female + write, data = df, trace = FALSE)\n\nCoefficients:\n         (Intercept) sesmiddle   seshigh femalefemale       write\nacademic   -2.853727 0.5190018 1.1472866  -0.07993539  0.05896798\nvocation    2.489106 0.9258690 0.3281005   0.52635457 -0.06559949\n\nStd. Errors:\n         (Intercept) sesmiddle   seshigh femalefemale      write\nacademic    1.166499 0.4506565 0.5229045    0.3967916 0.02236771\nvocation    1.184545 0.5015360 0.6628617    0.4629303 0.02507210\n\nResidual Deviance: 357.718 \nAIC: 377.718 \n\n\nThere are no \\(p\\)-values on this output but you can calculate them manually as explained in the IDRE UCLA example (see the footnote). That is not our issue now. We want to create a table for these results in a publishable format. And that is the tricky part. {Stargazer} provides one (recommended) method to do so:\n\nlibrary(stargazer)\nstargazer(multinom_fit, type = \"html\")\n\n\n\n\n\n\n\n\n\n\nDependent variable:\n\n\n\n\n\n\n\n\n\n\n\n\nacademic\n\n\nvocation\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n\n\n\n\n\n\nsesmiddle\n\n\n0.519\n\n\n0.926*\n\n\n\n\n\n\n(0.451)\n\n\n(0.502)\n\n\n\n\n\n\n\n\n\n\n\n\nseshigh\n\n\n1.147**\n\n\n0.328\n\n\n\n\n\n\n(0.523)\n\n\n(0.663)\n\n\n\n\n\n\n\n\n\n\n\n\nfemalefemale\n\n\n-0.080\n\n\n0.526\n\n\n\n\n\n\n(0.397)\n\n\n(0.463)\n\n\n\n\n\n\n\n\n\n\n\n\nwrite\n\n\n0.059***\n\n\n-0.066***\n\n\n\n\n\n\n(0.022)\n\n\n(0.025)\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-2.854**\n\n\n2.489**\n\n\n\n\n\n\n(1.166)\n\n\n(1.185)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n377.718\n\n\n377.718\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.1; p&lt;0.05; p&lt;0.01\n\n\n\n\n\nYou can also export the table to \\(\\LaTeX\\), but not Word (don’t forget to add results='asis' to code chunk if you use rmarkdown document). Next, you can use tab_model from {sjPlot}:\n\nlibrary(sjPlot)\ntab_model(multinom_fit)\n\n\n\n\n \nprog\n\n\nPredictors\nOdds Ratios\nCI\np\nResponse\n\n\n(Intercept)\n0.06\n0.01 – 0.58\n0.015\nacademic\n\n\nses [middle]\n1.68\n0.69 – 4.09\n0.251\nacademic\n\n\nses [high]\n3.15\n1.12 – 8.84\n0.029\nacademic\n\n\nfemale [female]\n0.92\n0.42 – 2.02\n0.841\nacademic\n\n\nwrite\n1.06\n1.01 – 1.11\n0.009\nacademic\n\n\n(Intercept)\n12.05\n1.16 – 124.67\n0.037\nvocation\n\n\nses [middle]\n2.52\n0.94 – 6.79\n0.066\nvocation\n\n\nses [high]\n1.39\n0.38 – 5.13\n0.621\nvocation\n\n\nfemale [female]\n1.69\n0.68 – 4.22\n0.257\nvocation\n\n\nwrite\n0.94\n0.89 – 0.98\n0.010\nvocation\n\n\nObservations\n200\n\n\nR2 / R2 adjusted\n0.124 / 0.119\n\n\n\n\n\n\n\ntab_model is great to create html tables. You can then drag and drop the html file to a Word document, and tweak a little to make it publication-ready. But it does not export to \\(\\LaTeX\\). Moreover, you might want a wide table (outcome categories side-by-side) rather than a long one.\nOther options are available, e.g., {texreg} package. It provides functions to export in all three formats mentioned above. But they create long tables.\n\nlibrary(texreg)\nhtmlreg(multinom_fit ,single.row=TRUE)\n\n\n\nStatistical models\n\n\n\n\n \n\n\nModel 1\n\n\n\n\n\n\nacademic: (Intercept)\n\n\n-2.85 (1.17)*\n\n\n\n\nacademic: sesmiddle\n\n\n0.52 (0.45)\n\n\n\n\nacademic: seshigh\n\n\n1.15 (0.52)*\n\n\n\n\nacademic: femalefemale\n\n\n-0.08 (0.40)\n\n\n\n\nacademic: write\n\n\n0.06 (0.02)**\n\n\n\n\nvocation: (Intercept)\n\n\n2.49 (1.18)*\n\n\n\n\nvocation: sesmiddle\n\n\n0.93 (0.50)\n\n\n\n\nvocation: seshigh\n\n\n0.33 (0.66)\n\n\n\n\nvocation: femalefemale\n\n\n0.53 (0.46)\n\n\n\n\nvocation: write\n\n\n-0.07 (0.03)**\n\n\n\n\nAIC\n\n\n377.72\n\n\n\n\nBIC\n\n\n410.70\n\n\n\n\nLog Likelihood\n\n\n-178.86\n\n\n\n\nDeviance\n\n\n357.72\n\n\n\n\nNum. obs.\n\n\n200\n\n\n\n\nK\n\n\n3\n\n\n\n\n\n\n***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05\n\n\n\n\n\n\nThis could do the job if you are okay with a long table but it requires some work. Unfortunately, I wanted a wide table with outcome categories on separate columns and I needed to export to Word.\nThe solution comes from the {modelsummary} package. Using the shape argument in the modelsummary function, we could create tables with outcome categories side-by-side.\n\nlibrary(modelsummary)\nlibrary(flextable)\nmodelsummary(multinom_fit,\n             stars = TRUE, exponentiate = TRUE, shape = model + term ~ response, \n            output = \"flextable\") %&gt;% \n  autofit() \n\n\n   academicvocation(1)(Intercept)0.058*12.051*(0.067)(14.274)sesmiddle1.6802.524+(0.757)(1.266)seshigh3.150*1.388(1.647)(0.920)femalefemale0.9231.693(0.366)(0.784)write1.061**0.937**(0.024)(0.023)+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\nIf you have more than one model, as usually is the case, you might want to specify shape argument in a different way:\n\nlibrary(modelsummary)\nlibrary(flextable)\n\nmultinom_fit1 &lt;- multinom(prog ~ ses + female + write, data = df, trace=FALSE)\nmultinom_fit2 &lt;- multinom(prog ~ ses + female + write + math, data = df, trace=FALSE)\n\nmodelsummary(list(multinom_fit1, multinom_fit2),\n             stars = TRUE, exponentiate = TRUE, shape = term ~ model + response, \n            output = \"flextable\") %&gt;% \n  autofit() \n\n\n (1) / academic(1) / vocation(2) / academic(2) / vocation(Intercept)0.058*12.051*0.012**42.394*(0.067)(14.274)(0.017)(64.709)sesmiddle1.6802.524+1.4362.700+(0.757)(1.266)(0.669)(1.367)seshigh3.150*1.3882.578+1.552(1.647)(0.920)(1.395)(1.044)femalefemale0.9231.6931.1671.480(0.366)(0.784)(0.480)(0.699)write1.061**0.937**1.0160.952+(0.024)(0.023)(0.027)(0.028)math1.075*0.959(0.030)(0.032)Num.Obs.200200R20.1240.165R2 Adj.0.1190.160AIC377.7365.0BIC410.7404.6RMSE0.420.41+ p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\nAs you can see, I also specify output argument as \"flextable\". Thanks to {modelsummary} package, we can use {gt}, {kableExtra}, {huxtable}, and {flextable} to customize the appearance of the table. I prefer {flextable} because it makes adding the table into a Word document quite easy as explained here.\nWe also need to make other changes: adding title, labels, spanner, confidence intervals, goodnes-of-fit statistics, etc.2 Luckily, there are various helper functions to make these changes. I highly recommend checking the websites for {modelsummary} and {flextable}.2 When you use odds ratios, and associated standard errors and confidence intervals, you need to be extra careful: https://github.com/tidymodels/broom/issues/422. For some, standard error of an odds ratio does not make much sense.\n\n\n\n\n\nReferences\n\nArel-Bundock, Vincent. 2021. Modelsummary: Summary Tables and Plots for Statistical Models and Data: Beautiful, Customizable, and Publication-Ready. https://CRAN.R-project.org/package=modelsummary.\n\n\nGohel, David. 2021. Flextable: Functions for Tabular Reporting. https://CRAN.R-project.org/package=flextable.\n\n\nHlavac, Marek. 2018. Stargazer: Well-Formatted Regression and Summary Statistics Tables. Bratislava, Slovakia: Central European Labour Studies Institute (CELSI). https://CRAN.R-project.org/package=stargazer.\n\n\nHugh-Jones, David. 2021. Huxtable: Easily Create and Style Tables for LaTeX, HTML and Other Formats. https://CRAN.R-project.org/package=huxtable.\n\n\nIannone, Richard, Joe Cheng, and Barret Schloerke. 2021. Gt: Easily Create Presentation-Ready Display Tables. https://CRAN.R-project.org/package=gt.\n\n\nLeifeld, Philip. 2013. “texreg: Conversion of Statistical Model Output in R to LaTeX and HTML Tables.” Journal of Statistical Software 55 (8): 1–24. http://dx.doi.org/10.18637/jss.v055.i08.\n\n\nLüdecke, Daniel. 2021. sjPlot: Data Visualization for Statistics in Social Science. https://CRAN.R-project.org/package=sjPlot.\n\n\nVenables, W. N., and B. D. Ripley. 2002. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nZhu, Hao. 2021. kableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra.\n\nCitationBibTeX citation:@online{2021,\n  author = {, T.E.G.},\n  title = {Trouble with {Multinomial} {Logistic} {Regression} {Tables}},\n  date = {2021-08-29},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nT.E.G. 2021. “Trouble with Multinomial Logistic Regression\nTables.” August 29, 2021."
  },
  {
    "objectID": "posts/clmb/index.html",
    "href": "posts/clmb/index.html",
    "title": "Maximum Likelihood Estimation: Finding the Top of a Hill",
    "section": "",
    "text": "I think one of the most intuitive descriptions of the maximum likelihood estimation (especially for the beginners) can be found in Long and Freese (2014):\n\nFor all but the simplest models, the only way to find the maximum likelihood function is by numerical methods 1. Numerical methods are the mathematical equivalent of how you would find the top of a hill if you were blindfolded and knew only the slope of the hill at the spot where you are standing and how the slope at that spot is changing which you could figure out by poking your foot in each direction. The search begins with start values corresponding to your location as you start your climb. From the start position, the slope of the likelihood function and the rate of change in the slope determine the next guess for the parameters. The process continues to iterate until the maximum of the likelihood function is found, called, convergence, and the resulting estimates are reported (Long and Freese 2014, 84)1 For a quick explanation of the difference between analytical and numerical methods: What’s the difference between analytical and numerical approaches to problems?\n\n\nExample: Logistic Regression\nData preparation\n\nlibrary(tidyverse)\nlibrary(optimx) # or optim depending on the optimization method used, \n                # BFGS is available in both packages\ndf &lt;- carData::Mroz\n\noutcome &lt;- fct_recode(df$lfp,\n               \"0\" = \"no\",\n               \"1\" = \"yes\")\noutcome &lt;- as.numeric(as.character(outcome))\n\npredictors &lt;- df %&gt;% \n  select(k5, age, inc) %&gt;%  # selected predictors\n  mutate(int=rep(1, nrow(df))) %&gt;% # column of 1s (intercept)\n  select(int, everything()) %&gt;% \n  as.matrix()\n\n“The search begins with start values corresponding to your location as you start your climb.”\n\n# Use OLS model coefficients as starting values\nlmfit &lt;- lm(outcome ~ predictors[,c(2:4)])\ns_val &lt;- lmfit$coefficients\n\n“From the start position, the slope of the likelihood function and the rate of change in the slope determine the next guess for the parameters.”\n\nlogLikelihood &lt;- function(vBeta, mX, vY) {\n  return(-sum(vY*(mX %*% vBeta - log(1+exp(mX %*% vBeta)))\n    + (1-vY)*(-log(1 + exp(mX %*% vBeta)))))  \n}\n\n“The process continues to iterate until the maximum of the likelihood function is found, called, convergence,…”\n\noptimization &lt;- optimx(s_val, logLikelihood, method = 'BFGS', \n                       mX = predictors, vY = outcome, hessian=TRUE)\n\n“…and the resulting estimates are reported.”\n\nestimation_optx &lt;- optimization %&gt;%\n  select(1:ncol(predictors)) %&gt;% t()\nestimation_optx\n\n                               BFGS\nX.Intercept.             3.39332484\npredictors...c.2.4..k5  -1.31311634\npredictors...c.2.4..age -0.05682900\npredictors...c.2.4..inc -0.01875491\n\n\nCompare them with the result of glm function:\n\nsummary(glm(lfp ~ k5 + age + inc, df, family = binomial))\n\n\nCall:\nglm(formula = lfp ~ k5 + age + inc, family = binomial, data = df)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.394398   0.515576   6.584 4.59e-11 ***\nk5          -1.313316   0.187535  -7.003 2.50e-12 ***\nage         -0.056855   0.010991  -5.173 2.31e-07 ***\ninc         -0.018751   0.006889  -2.722  0.00649 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1029.75  on 752  degrees of freedom\nResidual deviance:  956.75  on 749  degrees of freedom\nAIC: 964.75\n\nNumber of Fisher Scoring iterations: 4\n\n\nHere is the wiki page for the BFGS (Broyden–Fletcher–Goldfarb–Shanno algorithm) method, which “belongs to quasi-Newton methods, a class of hill-climbing optimization techniques….”\n\n\n\n\n\n\nReferences\n\nLong, Scott J., and Jeremy Freese. 2014. Regression Models for Categorical Dependent Variables Using Stata. Texas: Stata Press.\n\nCitationBibTeX citation:@online{2018,\n  author = {, T.E.G.},\n  title = {Maximum {Likelihood} {Estimation:} {Finding} the {Top} of a\n    {Hill}},\n  date = {2018-02-06},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nT.E.G. 2018. “Maximum Likelihood Estimation: Finding the Top of a\nHill.” February 6, 2018."
  },
  {
    "objectID": "posts/BvsB/index.html",
    "href": "posts/BvsB/index.html",
    "title": "Bourdieu and Boudon: Why One is Revered and the Other Neglected in Turkey?",
    "section": "",
    "text": "Figure 1: French sociologists content with themselves.\n\n\nLet’s start with a banal observation: Pierre Bourdieu is one of the most influential (French) sociologists in Turkey to date, if not the most influential. A quick search on the Dergipark1 reveals that there are 124 articles with “Bourdieu” keyword. Innumerable other articles focus on or use his ideas (so lots of habitus and capitals flying around). Almost all his major works were translated into Turkish. Journal issues were dedicated to him (e.g., 74th issue of Cogito or the latest issue of Strata). He is influential and celebrated. Years ago, a colleague joyously called him a “master key” which could open all the doors (of research or what? I should have asked.)1 Online repository of academic journals in Turkey.\nNow look at Raymond Boudon. If you search his name on Dergipark, you will get nothing. And I mean nothing: nothing on keywords or articles. You can find some vague references if you search google scholar in Turkish. Three of his books were translated into Turkish.2 And unsurprisingly, he is not well-known in academic or intellectual circles in Turkey, if known at all.2 Only one is still in print, Relativism, which is not available in English.\nGoing back to the original question, one might ask why we compare these two in the first place. Well, both were sociologists, French, born around the same time (4 years apart to be exact), and studied inequality, mobility and education, and they had some kind of rivalry. But most importantly, academics in Turkey love to import big names and their ideas. Whereas they have been very “productive” in the case of Bourdieu, they have neglected Boudon. Why?\nFor many, there is an easy answer: “Bourdieu is better.” And this could mean several things. It could mean that “Bourdieu has better explanations for social phenomena such as educational mobility”. “He provides a well-developed conceptual framework to analyze social inequality”. “He is way more prolific and successfully demonstrates the depth and breath of his theories in different studies.” The list could go on and I am sure a more knowledgeable Bourdieusian3 will come up with a longer/better one. So, people recognized this superiority and decided to follow the master writing about habitus, field, and capital. In other words, intrinsic qualities of Bourdieu’s work explain its prominence.43 Bourdieuian? Well, you figure it out.4 This line of reasoning might seem strange. It implies that people know Bourdieu and Boudon well enough to compare and reach a conclusion. This is highly unlikely. But it also works if we assume, for now, that intrinsic qualities are the reason why people know Bourdieu but not Boudon.\nAnother easy answer is that “Bourdieu is more popular,” and I partly agree. As I said, we love to import ideas and what attracts us more than fame? But I do not think that it explains the extent of appeal of Bourdieu or neglect of Boudon.55 I think the relationship between success and fame is not straightforward.\nAs you might guess, I am not convinced of these answers. However, I am not invested enough in this topic to turn it into a full-fledged analysis. So, I am going to speculate using an article: Michèle Lamont’s (1987) “How to Become a Dominant French Philosopher: The Case of Jacques Derrida.” Lamont discussed how Derrida became prominent in France, i.e., how he met cultural and institutional requirements of his intellectual milieu, and later gained acceptance in the US. This study might give us some clues about what happened in Turkey.\nI am not going to review or summarize this article. I am just going to take some arguments, present them in random order, and apply to my case. I hope this will lead to a more satisfactory answer.\n\nLet’s return to the master key metaphor. It basically refers to the adaptability of Bourdieusian sociology to specific intellectual and academic requirements in Turkey. And this could happen in two ways. First, Bourdieusian sociology can be adapted to distinct research interests. For instance, the diversity of theses and dissertations using Bourdieusian sociology in Turkey is worth noting. Apart from the well-known topics such as mobility and inequality, they cover military habitus, religious field, social media, entrepreneurship, Friday sermons, field of translation studies, etc.6 Second, Bourdieu can bring together those who have an explicit interest in either empirical research or social theory. If you look for a sophisticated theoretical apparatus, you can find it in Bourdieu.7 If you look for empirical studies with diverse methodological approaches, you can find it in Bourdieu as well. So, there is a Bourdieu for everyone.\nThe first point could lead us to the role of intellectual public. As I mentioned above, there is a lively interest in Bourdieu’s work in Turkey. It is even possible to identify a loose group of scholars mainly inspired by his work. It might be unfair to call them “Bourdieusian” but their brand of relational sociology is highly influenced by Bourdieu. These scholars might have a chance to build a distinct institutional/intellectual base8 for their brand of relational sociology but whether they could take this chance is unclear.\nRelated to the previous points, Bourdieusian concepts can easily be packaged (e.g., the troika: habitus, field, capital), and recognized. You can proliferate them even further (e.g., new forms of capital, new fields). These concepts circulate among scholars, and enable them to signal each other.\nBourdieusian sociology is also politically attractive. I do not mean that scholars find an expression of their (usually left) political views in his sociology. This could be the case. But essentially, Bourdieu shows them that they do not need to respond to urgent political matters while sacrificing sociological rigor. This works quite well with the public employee and critical intellectual image of academics in Turkey.\n\n6 And these are results from a title search only.7 The hallmark of social theory in Turkey is review articles on European sociologists in the form of “the concept of X according to Y.” His sociology is a fertile ground.8 Maybe, they bet on it. Who knows.Let’s turn to Boudon and our question: why is he unknown in Turkey? First of all, Boudon espouses (a form of) methodological individualism, uses examples from game theory, and critically engaged with rational choice theory. These are rather unfamiliar subjects for sociology audience in Turkey.9 Boudon’s theoretical apparatus cannot be easily packaged for this reason. He is definitely not the master key sought after by young sociologists: his work requires some knowledge of formal analysis and mathematics. Most sociologists (and social scientists in general) in Turkey do not have a strong grasp of them. It is hard to imagine qualitative studies inspired by his sociology. It is unlikely to have a “Boudonian” approach for many social phenomena10 that interest sociologists in Turkey. So, the fit between Bourdieusian sociology and academic/intellectual milieu in Turkey is absent in the case of Boudon. The cost of importing his ideas and advertising them is high, and the expected return is low.9 This is not the case for Bourdieu. I think sociologists are quite familiar with his intellectual precursors.10 And probably, for a good reason.\nTo sum up, the difference between the two cannot be explained solely by the intrinsic qualities or popularity. The fit between the sociological approach and intellectual milieu, the presence of a receptive public, and the familiarity with the wider theoretical framework (or lack of it) provide a better explanation.\nThere is one last thing I want to talk about: the reason why I asked this question. I think it is unfortunate that we do not know more about Boudon and a certain kind of sociology inspired by his (and others) work. This sociology is characterized by its emphasis on microfoundations, mechanisms, and explanation. Some proponents call it “rigorous” or “scientific” sociology. Well, these are bold statements and we don’t need to go that far.11 But it is a tradition neglected in Turkey with some valuable contributions.11 It is a risky business to call what you do as “scientific” while dismissing others.\n\n\n\n\n\nReferences\n\nLamont, Michèle. 1987. “How to Become a Dominant French Philosopher: The Case of Jacques Derrida.” American Journal of Sociology 93 (3): 584–622. https://doi.org/10.1086/228790.\n\nCitationBibTeX citation:@online{2022,\n  author = {, T.E.G.},\n  title = {Bourdieu and {Boudon:} {Why} {One} Is {Revered} and the\n    {Other} {Neglected} in {Turkey?}},\n  date = {2022-08-29},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nT.E.G. 2022. “Bourdieu and Boudon: Why One Is Revered and the\nOther Neglected in Turkey?” August 29, 2022."
  },
  {
    "objectID": "posts/socapp/index.html",
    "href": "posts/socapp/index.html",
    "title": "Sorcerer’s Apprentice Approach to Statistical Analysis",
    "section": "",
    "text": "Numbers do not produce value. The widespread availability of muscular number-crunching computers has had the untoward effect of yielding power to the sorcerer’s apprentice. Statistics and computers must support the research activity, not motivate it (Alfred 1987, 3).\n\nAlfred criticized neglecting the role of theory (which should guide any reseach inquiry) for the sake of “number-crunching.” This is a fair argument and repeated frequently by others. I would like to talk about a different (but related) issue using the metaphor, sorcerer’s apprentice. I am less concerned with the role of theory and its relation to method, but I am interested in how statistical methods are used in practice. But first, what does sorcerer’s apprentice mean?\nIt is the name of a poem by Goethe (Der Zauberlehrling 1). The story is about a sorcerer’s apprentice who is left alone to do some chores:1 I think the German word captures the diminutive sense better.\n\nTired of fetching water by pail, the apprentice enchants a broom to do the work for him – using magic in which he is not yet fully trained. The floor is soon awash with water, and the apprentice realizes that he cannot stop the broom because he does not know how. The apprentice splits the broom in two with an axe – but each of the pieces becomes a whole new broom that takes up a pail and continues fetching water, now at twice the speed.\n\nIn the end, the master returns and saves the apprentice 2. The moral of the story: one should not dare to do things beyond his/her understanding. In our version, apprentice is an academic 3, and the broom is “statistics and computers.” Yet, I think computers (or rather the software we use for analysis) could become “broom” alone. And it might lead to a lack of understanding of statistics. In fact, this is different from neglecting the role of theory and might happen even if someone has a strong theoretical background and guidance.2 You might not know that this is Goethe’s poem, but maybe the story sounds familiar. The reason is that you can find references to the Sorcerer’s Apprentice from the Communist Manifesto to Disney’s Fantasia (1940).3 It might mean faculty member or graduate student. Honestly, I don’t think it matters a lot here.\n\n\n\n\n\nDer Zauberlehrling by Ferdinand Barth (1842–1892)\n\n\n\n\n\nThere are various reasons why this could happen, especially in the social sciences 4. I would like to discuss three of them. The first and well-known reason (already stated by Alfred) is the availability of computers and increasing (computational) power of statistical software. Nowadays, we have easy access to the state-of-art programs which implement complex statistical techniques. The information on where to click or few lines of code are usually available online. The software returns an output and all you need to know is what those numbers in the output mean (I might be exaggerating). But the software availability alone is not enough and this takes us to the second (and more important) reason: the golems of the analysis.4 I think social sciences are particularly susceptible, but this could be an overstatement.\nIn his book, McElreath (2015) begins the first chapter with the example of statistical golems which refer to the “tests” widely used in statistical analyses and taught in introductory courses. Those of us who took these courses are familiar with these golems, for example:\n\nWhenever someone deploys even a simple statistical procedure, like a classical t-test, she is deploying a small golem that will obediently carry out an exact calculation, performing it the same way (nearly) every time, without complaint. (McElreath 2015, 2)\n\nIt is easy to find compilations of these golems, and guidelines on how to choose among them. A quick google search with “which statistical test to use” would return dozens of flowcharts (McElreath provided one in his book).\nIt is neither possible to deny the usefulness of these golems nor practical to get rid of them completely as they still might have some pedagogical value. But we need to keep in mind McElreath’s warning that “…there is no wisdom in the golem. It doesn’t discern when the context is inappropriate for its answers. It just knows its own procedure, nothing else” (McElreath 2015, 2). 5 So, it is not simply a matter of computational power and software implementation, but rather, how we use these two together. Unfortunately, the “know-how” about these golems might conceal the ignorance of the actual analytical techniques.5 The issue is not limited to this test and similar others. For example, in the introductory courses, we learn four scales of measurement (Stevens 1946): nominal, ordinal, interval, and ratio. Now imagine we have a variable showing number of children in a household and response categories are “0, 1, 2, 3, 4, 5+”. How would you classify such a variable based on the four scales above? It is not nominal, not interval or ratio (e.g., 5+). One can say ordinal here, but we might lose information by treating it ordinal, especially for the first response categories. A better category would be (right-) censored count variable which can be analyzed (as an outcome) using a censored poisson regression model (Cameron and Trivedi 2005).\nThese arguments seem to imply that the burden is on the shoulders of the researcher. To a certain extent, this is true: the researcher should act responsibly, and explain the procedures and rationale behind them. This requires giving some thought to the tests instead of treating them as ready-made solutions (golems). But there is a third reason as to why this is not always possible. And it is time and resource constraints.\nIt is expected that a graduate student should master their substantive research area and the statistical methods they use (or in general, research methods). But is this really possible? If so, to what extent? If a student does not come to graduate school with a strong background in statistics, most graduate courses/seminars would not take far beyond an intermediate level. For basic applications, this could be enough. But in practice, we have to deal with intricate problems which require expertise.66 One can argue that that is why we have academic advisors. But again, unless their substantive area of research includes statistical methods, can we really trust them? Are they more careful in using these statistical golems? I would prefer to err on the side of caution. So, unlike the sorcerer’s apprentice, we might not have a master to save the day.\nUsually, we need to prioritize what to study, especially under the pressure to finish projects and publish. Learning and mastering statistical methods can be time consuming and less rewarding. There is a trade-off and the decision is not that difficult: first and foremost, we are expected to contribute to our substantive area of research. There are lots of golems out there doing the job. And we saw others (senior academics?) using them without much hesitation. So lurks the danger of becoming a sorcerer’s apprentice.\nHere is my two cents on how to avoid this approach as much as possible:\n\nWe might not be able to master statistical methods, their mathematical representation, or their software implementation, but it is possible to learn, at least in simpler terms, what is going on behind the curtain. For instance, there are many accessible discussions on the assumptions of statistical techniques and what could happen in case of violation.\nClose attention to model specification would help as well. For example, one should be clear about the rationale of including some variables, while excluding others. It would be misleading to dump all variables, and then eliminating them one by one in an arbitrary fashion (e.g., p-value fishing). If we are not careful enough about the model specification, we will end up with “garbage in, garbage out” models.\nAfter running the model, it is good to keep your guard up and remain suspicious about the results, instead of accepting them as presented in the computer output. It is possible that you missed some important issue at some point in the previous steps of the analysis. Making sensible changes (but not fishing for significant results or larger effect sizes), and testing for sensitivity would be helpful to improve the quality of the analysis.\n\nNote: A friend commented that these arguments mostly apply to secondary data analysis. For example, in experimental research, there are other issues that require more attention (such as randomization, power, treatment, etc.), but they might not encounter the above mentioned problems to the same extent (e.g., variable selection, model specification). I agree with my friend, since I had secondary data analysis in my mind while writing this post. But I also think that the experimental researchers are no less susceptible to become a sorcerer’s apprentice. However, they might need to adopt different precautionary measures.\n\n\n\n\n\n\nReferences\n\nAlfred, Braxton M. 1987. Elements of Statistics for the Life and Social Sciences. London Paris Tokyo: Springer-Verlag.\n\n\nCameron, A. Colin, and Pravin K. Trivedi. 2005. Microeconometrics: Methods and Applications. Cambridge: Cambridge University Press.\n\n\nMcElreath, Richard. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan.\n\n\nStevens, S. S. 1946. “On the Theory of Scales of Measurement.” Science 103: 677–80.\n\nCitationBibTeX citation:@online{2018,\n  author = {, T.E.G.},\n  title = {Sorcerer’s {Apprentice} {Approach} to {Statistical}\n    {Analysis}},\n  date = {2018-12-07},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nT.E.G. 2018. “Sorcerer’s Apprentice Approach to Statistical\nAnalysis.” December 7, 2018."
  },
  {
    "objectID": "posts/aart/index.html",
    "href": "posts/aart/index.html",
    "title": "Achen ve Üç Kuralı",
    "section": "",
    "text": "Yazıya başlamadan önce başlığın Cristopher Achen’ın üç kuralı anlamına gelmediğini, Achen’ın “Toward A New Political Methodology: Microfoundations and ART.” (2002) isimli makalesinde bahsettiği Üç Kuralı’na (A Rule of Three) gönderme yaptığını belirtmeliyim. Yazıda bu makale ve Üç Kuralı üzerinde duracağım.1 Aslında Üç Kuralına makalenin sonlarında kısaca ve bir öneri olarak değiniliyor. Bu öneriye kadar Achen’ın yaptığı tartışmalar da ilginç. Kısaca bu bölümlere değinmek konuyla ilgili daha iyi bir fikir verebilir.1 Aslında düşününce Achen’ın üç kuralı demekte de bir sıkıntı yok.\nGirişi takip eden üç bölümde Achen bilinen tahminleyicilerin (estimator) genelleştirilmesi konusunu tartışıyor. Özellikle logit’in iki genelleştirmesi üzerinde duruyor: scobit (skewed logit) ve power logit. Nicel analizlere aşina olmayanlar için bu bölümler biraz ürkütücü gelebilir, çok sayıda yunan harfi ve formül havada uçuşuyor. Aslında arkadaki fikir oldukça basit. Mesela iki kategorili (“başarı” ve “başarısızlık”) bir değişken için bir logit modeli düşünelim. Eğer \\(P\\) bu model altında başarılı olma olasılığı ise,\n\\[P = \\frac{1}{1 + e^{-z}}\\] bize lojistik dağılımın kümülafif dağılım fonksiyonunu (cumulatif distribution function - cdf) verecektir. Genelde model bir link fonksiyonuyla tamamlanır yani \\(z\\) bağımsız değişkenlerin lineer fonksiyonu olarak ifade edilir (\\(z_i = X_i\\beta\\)). Achen, scobit ve power logit’in bu modelin bir genelleştirmesi olarak düşünülebileceğini söylüyor. Mesela,\n\\[P_{i}^{**} = P_{i}^{a} \\frac{1}{(1 + e^{-X_i\\beta})^\\alpha}\\] bize power logit cdf’sini verir. Achen neden böyle bir genelleştirmeye ihtiyaç duyabileceğimizi, logit modelini random utility model olarak ifade ettikten sonra tartışıyor.2 Ancak burada daha fazla ayrıntıya girip açıklamamıza gerek yok. Zaten Achen’ın amacı da bu tahminleyicileri okura tanıtmak değil. Daha ziyade sayıları gittikçe artan tahminleyicilerin gerekliliğini sorgulamak. Achen’a göre tahminleyicileri çoğaltmak sorun değil hatta tartışma sırasında iki tane (sahte) tahminleyici de kendisi uyduruyor (mixit ve clumpit). Sorun bu tahminleyicilerin istatiksel kesinliği (precision) düşürmesi. Mesela scobit, logit’e tek parametre ekliyor (\\(\\alpha\\)) ancak çok daha büyük örneklem gerektiriyor (Achen’ın örnek verdiği çalışmada 100.000’in üzerinde gözlem var). Achen, sadece eldeki veriye özgü bir durumu hesaba kattığı ya da daha genel olduğu için yeni bir tahminleyici kullanmayı sorunlu buluyor. Ayrıca sürekli daha genel tahminleyiciler üretmeye çalışmanın metodolojistlerin işi olmadığını söylüyor:2 Kimi zaman iki kategorili bağımlı değişkeni örtük (latent) bir değişken olarak modellemek de denir (örn., \\(y^*=X_i\\beta + u\\)). Burada Achen için önemli mesele gözlemlenmeyen stokastik değişken (hata (error) da denir) olan \\(u\\)’nun kümülatif dağılım fonksiyonunun simetrik olması. Böyle simetrik bir dağılımın yarattığı kısıtlamaları aşmak için scobit ya da power logit kullanılmasını tartışıyor (Achen 2002, 427–28).\n\nStatisticians do that for a living, and we will never be as good at their job as they are. Trying to keep up will leave us forever second-rate —at best— and, more importantly, irrelevant to genuine empirical advance in the discipline. (Achen 2002, 437)\n\nMakalenin bundan sonra ampirik ilerlemeyi mümkün kılacak iki güzergaha işaret ettiğini söyleyebiliriz. İlkinde Achen, tahminleyicileri seçmek ve yapılandırmak için (siyasal) aktörlerin davranışlarına dair formel modellere başvurulması gerektiğini söylüyor. Bu formel modelin hangi teoriden geldiği önemli değil. Önemli olan formel modelin, istatistiksel tanımlamla (specification) için mikrotemel olması. Daha kaba bir ifadeyle, istatiksel model, bu formel modelden çıkmalı. Buna göre bir tahminleyici formel modele uygunsa kullanılmalı yoksa sırf yeni, moda ya da karmaşık olduğu için tercih edilmemeli. Achen’ın ifade şekli kimilerine yabancı gelebilir. Ancak arkasındaki fikir oldukça tanıdık: teori (ve teoriden çıkan model) analizi yönlendirmeli.\nİkinci güzergah ise daha zorlu çünkü ilkinin aksine elimizde formel bir model yok. Achen çoğu durumda araştırmacıların (özellikle daha genç disiplinlerde) sınırlı teorik yönlendirmeyle çalışmak zorunda olduklarını hatırlatıyor. Böyle bir durumda Üç Kuralını (A Rule of Three) öneriyor:\n\nA Rule of Three (ART): A statistical specification with more than three explanatory variables is meaningless (Achen 2002, 446).\n\nYani Achen’a göre bir sürü kontrol değişkeniyle, karmaşık modeller hesaplamak yerine üç bağımsız değişkenle daha basit bir analiz yapmak, mesela çapraz tablolar kullanmak, çok daha aydınlatıcı olabilir. Hatta Achen bir adım daha ileri gidip örneklemi bölmeyi ve her alt-örneklem üzerinden ayrı ayrı analiz yapmayı öneriyor.3 Böylece gruplar arasındaki farkların (farklı hikayelerin) daha iyi yakalanabileceğini iddia ediyor.3 “If one needs several more controls, then there is too much going on in the sample for reliable inference. No one statistical specification can cope with the religious diversity of the American people with respect to abortion attitudes, for example. We have all done estimations like these, underestimating American differences and damaging our inferences by throwing everyone into one specification and using dummy variables for race and denomination. It’s easy, but it’s useless, and we need to stop.” (Achen 2002, 446)\nKısaca özetlemek gerekirse, Achen:\n\nTeorilerin ve bunlardan çıkan formel modellerin istatiksel analizi yönlendirmesi gerektiğini,\nBöyle bir formel model yoksa, analizin çok daha titiz ve tutumlu yapılması, bir sürü değişkenin analize rastgele atılmaması gerektiğini söylüyor ve üç kuralını öneriyor.\n\nİlk maddeye en azından kağıt üzerinde çoğu araştırmacının itiraz edeceğini sanmıyorum. Elbette böyle bir uygulama gerçekçi değil. Veri analizi pratikte çok daha karmaşık, kitaplarda sırayla takip edilmesi önerilen adımların içiçe geçtiği bir süreç. Bilimsel makalelerde, sanki öyle değilmişçesine, her şey kitabına uygun yapılmış gibi sunulması durumu değiştirmiyor.\nİkinci maddeye de bir yere kadar itiraz edileceğini düşünmüyorum, yani üç kuralına kadar. Öncelikle üç değişken tercihini keyfi bulanlar olacaktır. Achen da neden üçü seçtiğine dair pek ikna edici bir açıklama sunmuyor (deneyim; iki değişkenin az, dört değişkenin fazla olması?). Ancak kanımca Achen’ın derdi de insanları ikna etmek değil, dikkat çekmek. Makaleyi zaman zaman alaycı bir dille yazması da bunun göstergesi.4 Ayrıca örneklemi bölme fikri (başka bir tabirle stratification) herkes tarafından kabul görmeyecektir. Mesela böyle bir durumda (stratified) parametre tahminleri arasındaki farkı istatiksel olarak test etme imkanı ortadan kalkıyor.54 “At this point, no doubt, empirical investigators and methodologists accustomed to contemporary political science norms will object.”Look,” they will say, “this new Glockenspiel estimator may not have those frou-frou microfoundations you insist on, but it makes theoretical sense by my lights: It takes account of the yodeled nature of my dependent variable, which ordinary regression ignores. Plus it can be derived rigorously from the Cuckoo distribution. Besides, it fits better. The graphs are pretty, at least if not looked at too closely, and the likelihood ratio test rejects the ordinary regression fit at the 0.05 level. Theory-schmeary. Our job is to let the data decide. I’m going to use Glockenspiel. Anything else is choosing a poorer fit.” Nearly all of us methodologists have shared these views at some stage of our professional lives.” (Achen 2002, 440)5 Achen böyle bir eleştiriye araştırma amaçlarının farklı olabileceği (mesela karşılaştırma olmadığı) cevabını verebilir.\nYukarıda bahsettiğim muğlaklık dışında iki muhtemel sorun daha var: (1) üç kuralının farklı disiplinlerdeki karşılığı ve (2) uygulamada keyfi olarak yorumlanması. Achen metodoloji tartışmasını, başlıktan da anlaşılacağı üzere, siyaset bilimi özelinde yürütüyor. Mesela analiz biriminin ülkeler olduğu bir araştırmada, birden fazla ekonomik ya da idari gösterge kullanmak (örn. World Bank governance indicators) multicollinearity sorununa yol açabilir. Kural böyle bir durumda işlevsel olabilir. Ancak analiz biriminin bireyler olduğu bir araştırmada, sosyodemografik ya da sosyoekonomik kontrolleri kullanmamak (Achen’ın verdiği örneklerin aksine) eleştirilebilir. Mesela sosyoloji ve psikoloji araştırmalarında üç kuralına uymak adına önemli bir sosyodemografik faktörü dışarda bırakmak göze batacaktır. Tam bu noktada sorun, Achen’ın tasvir ettiği gibi formel modelle ilgili keskin bir ayrımın (var/yok) olmaması.\nDiğer mesele ise uygulamada keyfi yorumlama ihtimali. Mesela formel model olmamasına rağmen, literatürde önemli olduğu gösterilen bir faktörü üç kuralına uymak adına dışarıda bırakmak. Bir grup değişkenden neden üçünün seçildiğini, diğerlerinin dışarıda bırakıldığını ya da bir verinin neden belli bir şekilde bölündüğünü açıklamamak vs. Achen’ı takip edip bu keyfi yaklaşımı gösteren makaleler olup olmadığına ayrıca bakmak gerekiyor. Dolayısıyla elimde bir örnek olmadığını, bunu sadece bir risk olarak gördüğümü söylemeliyim.\nKısacası Achen’ın mesajının ruhunu geçerli bulsam da bunun kendini gösterdiği biçim (ART) tartışmalı gözüküyor. Kanımca daha genel olarak alınacak ders, veri analizinde Achen’ın gösterilmesini istediği titizliğin bu tür kuralları birebir uygulamayı zorlaştırması.\n\n\n\n\n\nReferences\n\nAchen, Christopher H. 2002. “Toward A New Political Methodology: Microfoundations and ART.” Annual Review of Political Science 5 (1): 423–50.\n\nCitationBibTeX citation:@online{2020,\n  author = {, T.E.G.},\n  title = {Achen Ve {Üç} {Kuralı}},\n  date = {2020-06-15},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nT.E.G. 2020. “Achen Ve Üç Kuralı.” June 15, 2020."
  },
  {
    "objectID": "posts/gb/index.html",
    "href": "posts/gb/index.html",
    "title": "Poor Man’s Galton Board",
    "section": "",
    "text": "Some of you might have seen a device called the “Galton Board” (also called the bean machine or quincunx) on social media, or more correctly, its desktop version by Four Pines Publishing. It got popular for a brief moment several months ago. Even Michael from Vsauce posted a video on it:\n\n\nThe device demonstrates central limit theorem, specifically how binomial distribution approximates to normal distribution. As you can see in the video, there are pegs on the board arranged in a triangular shape. You drop a single bean, the bean hits the peg and falls left or right with some probability (\\(p\\)). Since we assume that the device is constructed well (i.e., unbiased), we expect the bean goes both sides with equal probability, \\(p=1-p=q=0.5\\). This step is repeated for each row of pegs and the bean ends up in a (corresponding, rectangular) bin. If the probability of bouncing right is \\(p\\) (in our case, \\(0.5\\)), the number of rows is \\(N\\), and the number of times the bean bounces to right is \\(n\\), then the probability of the bean ending up in the \\(n\\)th bin from left is,\n\\[\\left( \\begin{array}{c} N \\\\ n \\end{array}\\right)=p^nq^{N-n},\\]\nwhich is probability mass function of a binomial distribution. Here is the catch: according to de Moivre-Laplace theorem (a special case of CLT), under certain conditions, this binomial distribution will approximate to the probability density function of a normal distribution with mean, \\(np\\) and variance \\(npq\\). In this case, if the number of rows (of pegs) and beans are large enough, the distribution would approximate to normal distribution, as the small Galton board (with 3000 beads and 12(?) rows of pegs) demonstrates.\nI really like this kind of small devices, but I am not willing to pay $39.95 (on Amazon). And, although the pleasure of watching the beans is missing, I can see the approximation at work using R:\n\nlibrary(tidyverse)\nlibrary(hrbrthemes)\nset.seed(12)\ndf &lt;- rbinom(3000, 12, 0.5)\n\ndf %&gt;% \n  data.frame() %&gt;% \n  ggplot(aes(.)) + \n  geom_histogram(aes(., stat(density)), binwidth = 1, color=\"white\") +\n  stat_function(fun=dnorm, color=\"black\", args=list(mean=mean(df), sd=sd(df))) +\n  scale_y_continuous(limits=c(0, 0.25), breaks = seq(0, 0.25, 0.05)) +\n  labs(title=\"Poor Man's Galton Board\") +\n  theme_ipsum_rc()\n\n\n\n\nMoreover, I can change the probability of bouncing to left or right, number of beans, and number of pegs (hence, bins) to see whether approximation works or not. (I also overlay a normal curve on histograms using sample mean and standard deviation.)\n\nTilting the Board\nIt is not hard to guess what would happen if I tilt the board to one side or the other. This will increase the probability of bouncing to left (or right) and we will end up with a skewed distribution.\n\n\n\n\n\n\n\nDecreasing the Number of Beans\nWhat would happen if I decrease the number of beans? On the left corner, we have the original board with 3000 beans and 12 pegs. Keeping the number of pegs constant, I decrease the number of beans to 1000, 500, and 100. I would say that the distribution of 1000 beans approximate the normal distribution quite well. But it is not the case for the distributions of 500 and 100 beans. One can see some skew, especially in the case of 100 beans.\n\n\n\n\n\n\n\nIncreasing the Number of pegs\nAnd if I increase the number of pegs (hence, the number of bins), the beans will spread more and more, and the distributions become platykurtic (see the change on x axis labels).\n\n\n\n\n\nThere is no way for us to know where a single bean would end up. But under certain conditions, it is possible to know the distribution of thousands of beans. This is what Galton1 (1889) called “Order in Apparent Chaos” (p.66) 2:1 Although he is an important figure in the history of statistics, nowadays Galton is criticized for his eugenics and “scientific racism.”2 Natural Inheritance is available here as PDF.\n\nI know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the “Law of Frequency of Error.” The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshalled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.\n\n\nUpdate 12 Dec, 2020:\nThere is a beautiful visualization of Galton Board on mikefc’s chipmunkcore repo:\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nGalton, Francis. 1889. Natural Inheritance. MacMillan.\n\n\nPedersen, Thomas Lin. 2017. Patchwork: The Composer of Ggplots. https://github.com/thomasp85/patchwork.\n\n\nRudis, Bob. 2019. Hrbrthemes: Additional Themes, Theme Components and Utilities for ’Ggplot2’. https://CRAN.R-project.org/package=hrbrthemes.\n\n\nWickham, Hadley. 2017. Tidyverse: Easily Install and Load the ’Tidyverse’. https://CRAN.R-project.org/package=tidyverse.\n\nCitationBibTeX citation:@online{2019,\n  author = {, T.E.G.},\n  title = {Poor {Man’s} {Galton} {Board}},\n  date = {2019-03-31},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nT.E.G. 2019. “Poor Man’s Galton Board.” March 31, 2019."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "About\nSociologist. Interested in quantitative analysis and statistical methods in the social sciences.\nCV: (html/pdf)."
  }
]