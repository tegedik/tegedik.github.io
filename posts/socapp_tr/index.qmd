---
title: "İstatistiksel Analize Büyücü Çırağı Yaklaşımı"
description: " "
author: 
  - name: T.E. Gedik
    url: https://tegedik.github.io
citation: true
date: "8/27/2025"
date-format: medium
draft: true
bibliography: ref.bib
reference-location: margin
lang: tr
---

---
nocite: | 
  @Alfred1987, @Cameron2005, @Stevens1946
---

<!--*I started writing this post months ago, but couldn't finish it. Now it seems redundant. Especially, after the ["Cargo-cult statistics and scientific crisis"](https://www.significancemagazine.com/593) by Stark and Satelli (2018), which was published in Significance Magazine and covers a lot more (I highly recommend it). Still, I forced myself to finish this and hope you enjoy it.*-->

> Sayılar değer üretmez. Güçlü ve yoğun hesaplama yapabilen bilgisayarların yaygınlaşması, büyücü çırağına güç verme gibi istenmeyen bir etkiye yol açmıştır. İstatistik ve bilgisayarlar araştırma faaliyetini desteklemelidir; onu motive etmemelidir. [@Alfred1987, s.3].

Alfred, herhangi bir araştırmaya rehberlik etmesi gereken teorinin  "sayı işleme" uğruna ihmal edilmesini eleştirmektedir. Bu haklı sav başkaları tarafından da sıkça tekrarlanmaktadır. *Büyücü çırağı* metaforunu kullanarak vurgu yapılan ve üzerinde durmak istediğim yöntemlerin beraberinde getirdiği güç ve sorunlar. Diğer bir ifadeyle, teorinin rolü ve metodla ilişkisi konusunda daha az endişeliyim ancak özellikle yazılımlarla daha kolay erişilir hale gelen istatistiksel tekniklerin pratikte mevcut sorunları şiddetlendirdiğini düşünüyorum. Hatta programları kullanma alışkanlıkları kimi zaman istatistiğin anlaşılmasının önünde bir engel teşkil edebiliyor.  Fakat önce, büyücünün çırağı ne anlama geliyor?

*Büyücü Çırağı*, Goethe'nin bir şiirinin adı (*Der Zauberlehrling* [^1]). [Hikaye](https://en.wikipedia.org/wiki/The_Sorcerer%27s_Apprentice) bazı ayak işlerini yapması için yalnız bırakılan bir büyücü çırağı hakkında:

[^1]: Bence Almanca kelime küçültme anlamını daha iyi yansıtıyor.

> Kova ile su taşımaktan bıkan çırak, henüz tam olarak öğrenmediği bir büyü kullanarak odadaki süpürgeyi işi yapması için canlandırır. Fakat odayı kısa sürede su basar ve çırak süpürgeyi [büyüyle] durduramayacağını fark eder. Bunun üzerine süpürgeyi baltayla ikiye ayırır, ancak her iki parça da ellerine kova alıp su taşımaya devam eden yeni birer süpürgeye dönüşür. Artık iki kat hızlı su taşınmaktadır.

Sonunda büyücü geri döner ve çırağı kurtarır.[^2] Kıssadan hisse: Kişi, kendi anlayışının ötesinde işler yapmaya kalkışmamalıdır. Bizim için çırak bir akademisyen,[^3] süpürge ise "istatistik ve bilgisayarlar". Hatta bilgisayarların (ya da daha doğrusu analiz için kullandığımız yazılımların) tek başına "süpürge" haline gelebileceğini düşünüyorum. Ve bu, yukarıda da yazdığım gibi, istatistiğin anlaşılmasını güçleştirebilir. Bu durum teorinin rolünü ihmal etmekten farklı ve kişinin güçlü bir teorik altyapısı olsa bile gerçekleşebilir.


[^2]: Goethe'nin şiiri olduğunu bilmeseniz de hikaye size tanıdık gelebilir. Bunun nedeni, Komünist Manifesto'dan [Disney'in *Fantasia* (1940)](https://youtu.be/2DX2yVucz24?t=26s) filmine kadar birçok yerde Büyücünün Çırağı'na atıfta bulunulmasıdır.

[^3]: Burada kişi, bir öğretim üyesi veya lisansüstü öğrencisi olabilir. Bir önemi olduğunu düşünmüyorum.

```{r, echo=FALSE, fig.align="center", fig.cap="Ferdinand Barth'ın (1842–1892) *Der Zauberlehrling* adlı çalışması"}
knitr::include_graphics("sa.png")
```

<!-- ![*Der Zauberlehrling* by Ferdinand Barth (1842–1892)](sa.png){width=100px} -->

There are various reasons why this could happen, especially in the social sciences [^4]. I would like to discuss three of them. The first and well-known reason (already stated by Alfred) is the availability of computers and increasing (computational) power of statistical software. Nowadays, we have easy access to the state-of-art programs which implement complex statistical techniques. The information on where to click or few lines of code are usually available online. The software returns an output and all you need to know is what those numbers in the output mean (I might be exaggerating). But the software availability alone is not enough and this takes us to the second (and more important) reason: the *golems* of the analysis.

[^4]: I think social sciences are particularly susceptible, but this could be an overstatement.

In his book, McElreath [-@McElreath2015] begins the first chapter with the example of *statistical golems* which refer to the "tests" widely used in statistical analyses and taught in introductory courses. Those of us who took these courses are familiar with these golems, for example:

> Whenever someone deploys even a simple statistical procedure, like a classical *t*-test, she is deploying a small golem that will obediently carry out an exact calculation, performing it the same way (nearly) every time, without complaint. [@McElreath2015, p.2]

It is easy to find compilations of these golems, and guidelines on how to choose among them. A quick google search with ["which statistical test to use"](https://www.google.com.tr/search?biw=1422&bih=737&tbm=isch&sa=1&ei=vJ8lW-PTFJKTmgWA9KXwBg&q=which+statistical+test+to+use+&oq=which+statistical+test+to+use+&gs_l=img.3..0i67k1j0l3j0i30k1j0i24k1l5.419359.423511.0.423693.22.19.0.0.0.0.231.2160.0j13j2.15.0....0...1c.1.64.img..8.14.2051....0.7I2-ZKpUkSM) would return dozens of flowcharts (McElreath provided one in his book).

It is neither possible to deny the usefulness of these golems nor practical to get rid of them completely as they still might have some pedagogical value. But we need to keep in mind McElreath's warning that "...there is no wisdom in the golem. It doesn't discern when the context is inappropriate for its answers. It just knows its own procedure, nothing else" [@McElreath2015, p.2]. [^5] So, it is not simply a matter of computational power and software implementation, but rather, how we use these two together. Unfortunately, the "know-how" about these golems might conceal the ignorance of the actual analytical techniques.

[^5]: The issue is not limited to this test and similar others. For example, in the introductory courses, we learn four scales of measurement [@Stevens1946]: nominal, ordinal, interval, and ratio. Now imagine we have a variable showing number of children in a household and response categories are "0, 1, 2, 3, 4, 5+". How would you classify such a variable based on the four scales above? It is not nominal, not interval or ratio (e.g., 5+). One can say ordinal here, but we might lose information by treating it ordinal, especially for the first response categories. A better category would be (right-) censored count variable which can be analyzed (as an outcome) using a censored poisson regression model [@Cameron2005].

These arguments seem to imply that the burden is on the shoulders of the researcher. To a certain extent, this is true: the researcher should act responsibly, and explain the procedures and rationale behind them. This requires giving some thought to the tests instead of treating them as ready-made solutions (*golems*). But there is a third reason as to why this is not always possible. And it is *time and resource constraints*.

It is expected that a graduate student should master their substantive research area *and* the statistical methods they use (or in general, research methods). But is this really possible? If so, to what extent? If a student does not come to graduate school with a strong background in statistics, most graduate courses/seminars would not take far beyond an intermediate level. For basic applications, this could be enough. But in practice, we have to deal with intricate problems which require expertise.[^6]

[^6]: One can argue that that is why we have academic advisors. But again, unless their substantive area of research includes statistical methods, can we really trust them? Are they more careful in using these statistical golems? I would prefer to err on the side of caution. So, unlike the sorcerer's apprentice, we might not have a master to save the day.

Usually, we need to prioritize what to study, especially under the pressure to finish projects and publish. Learning and mastering statistical methods can be time consuming and less rewarding. There is a trade-off and the decision is not that difficult: first and foremost, we are expected to contribute to our substantive area of research. There are lots of golems out there doing the job. And we saw others (senior academics?) using them without much hesitation. So lurks the danger of becoming a *sorcerer's apprentice*.

Here is my two cents on how to avoid this approach as much as possible:

<!--Learning what is going on behind the curtain (assumptions, formulae, basic math) at least to have a general idea:-->

We might not be able to master statistical methods, their mathematical representation, or their software implementation, but it is possible to learn, at least in simpler terms, what is going on behind the curtain. For instance, there are many accessible discussions on the assumptions of statistical techniques and what could happen in case of violation.

Close attention to model specification would help as well. For example, one should be clear about the rationale of including some variables, while excluding others. It would be misleading to dump all variables, and then eliminating them one by one in an arbitrary fashion (e.g., p-value fishing). If we are not careful enough about the model specification, we will end up with "garbage in, garbage out" models.

After running the model, it is good to keep your guard up and remain suspicious about the results, instead of accepting them as presented in the computer output. It is possible that you missed some important issue at some point in the previous steps of the analysis. Making sensible changes (but *not* fishing for significant results or larger effect sizes), and testing for sensitivity would be helpful to improve the quality of the analysis.

<!--Finally, there is nothing wrong about asking for help or feedback. Even if it comes from a colleague who could make similar mistakes, a new perspective might help us to see a problem *hiding in plain sight*.-->

*Note: A friend commented that these arguments mostly apply to secondary data analysis. For example, in experimental research, there are other issues that require more attention (such as randomization, power, treatment, etc.), but they might not encounter the above mentioned problems to the same extent (e.g., variable selection, model specification). I agree with my friend, since I had secondary data analysis in my mind while writing this post. But I also think that the experimental researchers are no less susceptible to become a sorcerer's apprentice. However, they might need to adopt different precautionary measures.*

<!-- [^1]: For those who need an authoritative word from French men: "Those who expect miracles from the mythical triad, *archives*, *data*, and *computers*, do not understand the difference between the constructed objects which are scientific facts...and the real objects which are observed in museums and which, trough their 'concrete surplus', offer unlimited possibilites for new constructions as new questions are put to them. If these epistemological prelimiaries are ignored, there is a great risk of treating identical things differently and different things identically, of comparing incomparable and failing to compare the comparable, because even the most objective 'data' are obtained by applying grids...which involve theoretical presuppositions and therefore overlook information which another construction of the facts might have grasped"[@Bourdieu1991, p.36]. -->
