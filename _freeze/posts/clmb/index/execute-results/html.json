{
  "hash": "495fada13bfe35d866e6e49a0ba4403d",
  "result": {
    "markdown": "---\ntitle: \"Maximum Likelihood Estimation: Finding the Top of a Hill\"\ndescription: \" \"\nauthor: \n  - name: Tahir Enes Gedik\n    url: {}\ncitation: true\ndate: \"2/6/2018\"\ndate-format: medium\ndraft: false\nbibliography: ref.bib\n---\n\n\nI think one of the most intuitive descriptions of the maximum likelihood estimation (especially for the beginners) can be found in @Long2014:\n\n> For all but the simplest models, the only way to find the maximum likelihood function is by numerical methods ^[For a quick explanation of the difference between analytical and numerical methods: [What’s the difference between analytical and numerical approaches to problems?](https://math.stackexchange.com/a/935458)]. Numerical methods are the mathematical equivalent of how you would find the top of a hill if you were blindfolded and knew only the slope of the hill at the spot where you are standing and how the slope at that spot is changing which you could figure out by poking your foot in each direction. The search begins with start values corresponding to your location as you start your climb. From the start position, the slope of the likelihood function and the rate of change in the slope determine the next guess for the parameters. The process continues to iterate until the maximum of the likelihood function is found, called, convergence, and the resulting estimates are reported [@Long2014, pp.84]\n\n### Example: Logistic Regression\n\nData preparation\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(optimx) # or optim depending on the optimization method used, \n                # BFGS is available in both packages\ndf <- carData::Mroz\n\noutcome <- fct_recode(df$lfp,\n               \"0\" = \"no\",\n               \"1\" = \"yes\")\noutcome <- as.numeric(as.character(outcome))\n\npredictors <- df %>% \n  select(k5, age, inc) %>%  # selected predictors\n  mutate(int=rep(1, nrow(df))) %>% # column of 1s (intercept)\n  select(int, everything()) %>% \n  as.matrix()\n```\n:::\n\n\"The search begins with *start values* corresponding to your location as you start your climb.\"\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use OLS model coefficients as starting values\nlmfit <- lm(outcome ~ predictors[,c(2:4)])\ns_val <- lmfit$coefficients\n```\n:::\n\n\"From the start position, the slope of the *likelihood function* and the rate of change in the slope determine the next guess for the parameters.\"\n\n::: {.cell}\n\n```{.r .cell-code}\nlogLikelihood <- function(vBeta, mX, vY) {\n  return(-sum(vY*(mX %*% vBeta - log(1+exp(mX %*% vBeta)))\n    + (1-vY)*(-log(1 + exp(mX %*% vBeta)))))  \n}\n```\n:::\n\n\"The process continues to *iterate* until the maximum of the likelihood function is found, called, *convergence*,...\"\n\n::: {.cell}\n\n```{.r .cell-code}\noptimization <- optimx(s_val, logLikelihood, method = 'BFGS', \n                       mX = predictors, vY = outcome, hessian=TRUE)\n```\n:::\n\n\"...and the resulting estimates are reported.\"\n\n::: {.cell}\n\n```{.r .cell-code}\nestimation_optx <- optimization %>%\n  select(1:ncol(predictors)) %>% t()\nestimation_optx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                               BFGS\nX.Intercept.             3.39332484\npredictors...c.2.4..k5  -1.31311634\npredictors...c.2.4..age -0.05682900\npredictors...c.2.4..inc -0.01875491\n```\n:::\n:::\n\nCompare them with the result of `glm` function:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm(lfp ~ k5 + age + inc, df, family = binomial))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = lfp ~ k5 + age + inc, family = binomial, data = df)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-1.867  -1.184   0.731   1.003   1.970  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  3.394398   0.515576   6.584 4.59e-11 ***\nk5          -1.313316   0.187535  -7.003 2.50e-12 ***\nage         -0.056855   0.010991  -5.173 2.31e-07 ***\ninc         -0.018751   0.006889  -2.722  0.00649 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1029.75  on 752  degrees of freedom\nResidual deviance:  956.75  on 749  degrees of freedom\nAIC: 964.75\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\nHere is the [wiki](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) page for the `BFGS` (Broyden–Fletcher–Goldfarb–Shanno algorithm) method, which \"belongs to quasi-Newton methods, a class of *hill-climbing* optimization techniques....\"\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}